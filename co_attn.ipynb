{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Project Overview**\n",
        "\n",
        "This project involves the use of a finetuned LLaMA model on antibiotic and bacteria information to assist doctors in analyzing antibiotic resistance based on mutation data from bacterial isolates. The process integrates both numerical (0/1) data for isolates and text-based insights from a language model. The pipeline combines these two types of information to improve the accuracy of antibiotic resistance prediction.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1HK5CzzUnC3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ipdb transformers torch pandas numpy accelerate"
      ],
      "metadata": {
        "id": "4cHHQU5kbcMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yewnKgwwbTS3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Pipeline Steps**\n",
        "\n",
        "1. **Configuration Setup and Data Loading**\n",
        "   - The project starts by loading configurations for the model, including hyperparameters and paths for data files. This configuration helps streamline adjustments during experimentation."
      ],
      "metadata": {
        "id": "cxf5g_6CnNeR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_config(config_file=None):\n",
        "    if config_file:\n",
        "        with open(config_file, 'r') as f:\n",
        "            config = yaml.safe_load(f)\n",
        "    else:\n",
        "        config = {\n",
        "            \"model_name\": \"microsoft/biogpt\",\n",
        "            \"learning_rate\": 1e-5,\n",
        "            \"epochs\": 2,\n",
        "            \"beta_kl\": 0.1,\n",
        "            \"alpha\": 1.0,\n",
        "            \"data_file\": \"/workspace/train.csv\"\n",
        "        }\n",
        "    return config"
      ],
      "metadata": {
        "id": "QgrHs7EsbZed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "2. **Training Stage I: Stylistic Token Generation**\n",
        "   - The LLaMA model is initially trained to produce stylistic tokens, aiming to generate responses relevant to pharmacology. This training uses KL divergence to compare outputs between the model and a reference model, encouraging the model to learn specialized vocabulary and context for antibiotic resistance.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wylfgsAfnWO3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stage I: Train initial biogpt model to produce stylistic tokens using the KL Div Loss\n",
        "def train_style(model, tokenizer, reference_model, data, epochs=2, lr=1e-5, beta_kl=0.1):\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
        "    model.train()\n",
        "    reference_model.eval()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0.0\n",
        "        for example in data:\n",
        "            conversation = stage1_chat_format(example)\n",
        "            conversation_text = \" \".join([f\"{msg['role']}: {msg['content']}\" for msg in conversation])\n",
        "            inputs = tokenizer(conversation_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "            inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "            generated_text = tokenizer.decode(outputs.logits.argmax(dim=-1)[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "            instruction = \"You are going to become an expert in pharamacy and adverse effects on medication. Learn to generate stylistic tokens for the pharmacology domain \\n\"\n",
        "            # Calculate KL divergence between model output and reference model\n",
        "            with torch.no_grad():\n",
        "                ref_outputs = reference_model(**inputs)\n",
        "                ref_log_probs = F.log_softmax(ref_outputs.logits, dim=-1)\n",
        "            log_probs = F.log_softmax(outputs.logits, dim=-1)\n",
        "            kl_loss = F.kl_div(log_probs, ref_log_probs, reduction='batchmean')\n",
        "\n",
        "\n",
        "            labels = inputs['input_ids'].clone()\n",
        "            # Set labels to -100 for the parts we don't want to compute loss for (user question, original answer, instruction)\n",
        "            labels[:, :len(inputs['input_ids'][0]) + len(tokenizer(instruction, return_tensors=\"pt\")['input_ids'][0])] = -100\n",
        "            modified_outputs = model(**inputs, labels=modified_labels)\n",
        "            cross_entropy_loss = outputs.loss\n",
        "\n",
        "            total_loss_value = cross_entropy_loss + beta_kl * kl_loss\n",
        "            optimizer.zero_grad()\n",
        "            total_loss_value.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += total_loss_value.item()\n",
        "        print(f\"Stage I - Epoch {epoch+1}, Loss: {total_loss:.4f}\")"
      ],
      "metadata": {
        "id": "dDCVyDVebT1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stage1_chat_format(example):\n",
        "    return [\n",
        "        {\"role\": \"user\", \"content\": example['question']},\n",
        "        {\"role\": \"assistant\", \"content\": example.get('original_answer', '')}\n",
        "    ]"
      ],
      "metadata": {
        "id": "KHg6u_UHd6fS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Initial Answer Generation**\n",
        "   - Once the model has been trained for style, it generates initial answers based on the questions provided in the dataset. These answers are used as a base to add context to the text data and are saved for future stages.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3TiYg7tAnpUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_initial_answers(model, tokenizer, data, output_file=\"/workspace/train.csv\", batch_size=16):\n",
        "    model.eval()\n",
        "    generated_data = []\n",
        "    for i in tqdm(range(0, len(data), batch_size), desc=\"Generating initial answers\"):\n",
        "        batch = data[i:i+batch_size]\n",
        "        questions = [example['question'] for example in batch]\n",
        "        inputs = tokenizer(questions, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(**inputs, max_new_tokens=50, eos_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "        for idx, example in enumerate(batch):\n",
        "            generated_answer = tokenizer.decode(outputs[idx], skip_special_tokens=True)\n",
        "            example['original_answer'] = generated_answer[len(example['question']):].strip()\n",
        "            example[\"correct_answer\"] = example[\"correct_answer\"]\n",
        "            generated_data.append(example)\n",
        "\n",
        "    df = pd.DataFrame(generated_data)\n",
        "    df.to_csv(output_file, index=False)\n",
        "    print(f\"Initial answers generated and saved to {output_file}\")"
      ],
      "metadata": {
        "id": "h9rP-uynebEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **Data Preparation**\n",
        "   - A dataset class is created to handle antibiotic resistance data. It includes two sets of features:\n",
        "     - A numerical vector (0/1) indicating the presence or absence of mutations.\n",
        "     - Encoded text features that add contextual information about antibiotic resistance, extracted from the LLaMA model output."
      ],
      "metadata": {
        "id": "boRJzwKantA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np\n",
        "\n",
        "class AntibioticResistanceDataset(Dataset):\n",
        "    def __init__(self, isolate_features, text_features, labels):\n",
        "        self.isolate_features = isolate_features\n",
        "        self.text_features = text_features\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        isolate_feature = self.isolate_features[idx]\n",
        "        text_feature = self.text_features[idx]\n",
        "        label = self.labels[idx]\n",
        "        return isolate_feature, text_feature, label"
      ],
      "metadata": {
        "id": "0IjG28-O9_3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "5. **Co-Attention Mechanism**\n",
        "   - A Co-Attention module is introduced to combine numerical isolate features with text-based features, helping the model to attend jointly to both sets of information. This fusion aims to leverage the strengths of both numeric and language data for more accurate predictions."
      ],
      "metadata": {
        "id": "aNaP_Ymin0zv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Co-Attention Module\n",
        "class CoAttention(nn.Module):\n",
        "    def __init__(self, isolate_dim, text_dim, hidden_dim):\n",
        "        super(CoAttention, self).__init__()\n",
        "        self.isolate_fc = nn.Linear(isolate_dim, hidden_dim)\n",
        "        self.text_fc = nn.Linear(text_dim, hidden_dim)\n",
        "        self.attention = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, isolate_features, text_features):\n",
        "        # Project both features to a common hidden space\n",
        "        isolate_proj = torch.relu(self.isolate_fc(isolate_features))\n",
        "        text_proj = torch.relu(self.text_fc(text_features))\n",
        "        attention_scores = torch.relu(self.attention(isolate_proj + text_proj))\n",
        "        attention_weights = self.softmax(attention_scores)\n",
        "        co_attended_features = attention_weights * (isolate_proj + text_proj)\n",
        "        return co_attended_features"
      ],
      "metadata": {
        "id": "auf3PTz8gW-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "6. **Antibiotic Resistance Prediction Model**\n",
        "   - A neural network model is built, which includes the co-attention mechanism followed by fully connected layers. This model takes the co-attended features and outputs a probability that predicts antibiotic resistance."
      ],
      "metadata": {
        "id": "ejxiYpgnn5H9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AntibioticResistancePredictor(nn.Module):\n",
        "    def __init__(self, isolate_dim, text_dim, hidden_dim):\n",
        "        super(AntibioticResistancePredictor, self).__init__()\n",
        "        self.co_attention = CoAttention(isolate_dim, text_dim, hidden_dim)\n",
        "        self.fc1 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, isolate_features, text_features):\n",
        "        co_attended_features = self.co_attention(isolate_features, text_features)\n",
        "        x = torch.relu(self.fc1(co_attended_features))\n",
        "        x = self.bn1(x)\n",
        "        x = self.dropout(x)\n",
        "        x = torch.sigmoid(self.fc2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "OXiXMev-gjGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. **Model Training**\n",
        "   - The model is trained on the combined data using binary cross-entropy loss. This helps in minimizing the difference between predicted and true resistance labels.\n"
      ],
      "metadata": {
        "id": "eZYoccQrn8Sy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the Model\n",
        "def train_co_attn(model, dataloader, criterion, optimizer, num_epochs=20):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for isolate_features, text_features, labels in dataloader:\n",
        "            isolate_features = isolate_features.float()\n",
        "            text_features = text_features.float()\n",
        "            labels = labels.float().view(-1, 1)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(isolate_features, text_features)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * isolate_features.size(0)\n",
        "        epoch_loss = running_loss / len(dataloader.dataset)\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "\n",
        "    model.save_pretrained(\"./trained_for_amr\")\n",
        "    tokenizer.save_pretrained(\"./trained_for_amr\")"
      ],
      "metadata": {
        "id": "Y1-1zaC_gUfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. **Textual Context Generation and Encoding**\n",
        "   - Context for each antibiotic and isolate combination is generated using the LLaMA model in a specific textual format. The output is encoded using a MedCPT encoder to extract text features, which are then used in the combined model."
      ],
      "metadata": {
        "id": "KnWf4u8Dn-Ui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate Context for textual encoding\n",
        "def generate_context(model, tokenizer, data_initial):\n",
        "    results = []\n",
        "    for item in data_initial:\n",
        "        input_text = f\"bacteria: {item['question']}, antibiotic: {item['correct_answer']}, affected_isolates: _______\\ngive a comprehensive analysis of the situation? \\n\"\n",
        "        inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "        output = model.generate(**inputs, max_length=256)\n",
        "        result = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "        results.append(result)\n",
        "    pd.DataFrame(results, columns=['generated_analysis']).to_csv(\"train.csv\", index=False)\n",
        "\n",
        "# Encode Text Features using MedCPT Encoder\n",
        "def encode_text_features_mecpt(model, tokenizer, data):\n",
        "    encoded_features = []\n",
        "    for item in data:\n",
        "        inputs = tokenizer(item['generated_analysis'], return_tensors=\"pt\").to(model.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            encoded_features.append(outputs.last_hidden_state.mean(dim=1).cpu().numpy())\n",
        "    return np.vstack(encoded_features)"
      ],
      "metadata": {
        "id": "o039d90fi8-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. **Inference**\n",
        "   - The trained model is used to predict resistance labels for new data, using both isolate features and encoded text features."
      ],
      "metadata": {
        "id": "GdQaRAwWoB31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(model, tokenizer, isolate_features, text_features):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        isolate_features = torch.tensor(isolate_features).float().to(model.device)\n",
        "        text_features = torch.tensor(text_features).float().to(model.device)\n",
        "        outputs = model(isolate_features, text_features)\n",
        "        predictions = (outputs > 0.5).cpu().numpy().astype(int)  # Convert probabilities to binary predictions\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "fPdjZyLWlP6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(config_file=None):\n",
        "    config = load_config(config_file)\n",
        "    model_name = config[\"model_name\"]\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name).to(torch.device(\"cuda\"))\n",
        "    model.config.pad_token = tokenizer.pad_token\n",
        "    data_file_path = config[\"data_file\"]\n",
        "    df = pd.read_csv(data_file_path)\n",
        "\n",
        "    # Prepare the data for generating initial answers\n",
        "    data_initial = df[[\"question\", \"correct_answer\"]].to_dict(orient=\"records\")\n",
        "\n",
        "    generate_initial_answers(model, tokenizer, data_initial)\n",
        "    df_generated = pd.read_csv(\"train.csv\")\n",
        "    data = df_generated[[\"question\", \"original_answer\", \"correct_answer\"]].to_dict(orient=\"records\")\n",
        "\n",
        "    # Style training\n",
        "    reference_model = AutoModelForCausalLM.from_pretrained(model_name).to(model.device)  # Load reference model for KL\n",
        "    train_style(\n",
        "        model, tokenizer, reference_model, data,\n",
        "        epochs=config[\"epochs\"],\n",
        "        lr=config[\"learning_rate\"],\n",
        "        beta_kl=config[\"beta_kl\"]\n",
        ")\n",
        "\n",
        "    model.save_pretrained(\"./trained_self_correcting_model\")\n",
        "    tokenizer.save_pretrained(\"./trained_self_correcting_model\")\n",
        "\n",
        "\n",
        "    data = generate_context(model, tokenizer, data_initial)\n",
        "    text_features = encode_text_features_mecpt(model, tokenizer, data)\n",
        "\n",
        "    isolate_features_df = pd.read_csv(\"amr_ast_clindamycin_CJ.csv\")\n",
        "    isolate_features = isolate_features_df.values\n",
        "    labels = isolate_features_df['label'].values\n",
        "\n",
        "    dataset = AntibioticResistanceDataset(isolate_features, text_features, labels)\n",
        "    dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "    isolate_dim = 50\n",
        "    text_dim = 768\n",
        "    hidden_dim = 128\n",
        "\n",
        "    model = AntibioticResistancePredictor(isolate_dim, text_dim, hidden_dim)\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    train_co_attn(model, dataloader, criterion, optimizer, num_epochs=20)\n",
        "\n",
        "    test_isolate_features = isolate_features[:5]\n",
        "    test_text_features = text_features[:5]\n",
        "    predictions = inference(model, tokenizer, test_isolate_features, test_text_features)\n",
        "    print(\"Predictions:\", predictions)\n"
      ],
      "metadata": {
        "id": "YPThDSHYeY6r"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "-_hU01w0fHZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Summary**\n",
        "\n",
        "The project integrates deep learning techniques and language models to analyze antibiotic resistance. It combines numerical mutation data with text-based context generated by a finetuned LLaMA model, enhancing the prediction of resistance using a co-attention mechanism. The system aims to provide doctors with comprehensive analysis by merging structured data with contextual understanding."
      ],
      "metadata": {
        "id": "UhPIE5CGoIFd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MOzuZzwzfJEv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}